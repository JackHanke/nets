{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpy settings\n",
    "np.random.seed(1)\n",
    "np.set_printoptions(suppress=True, linewidth = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# MNIST dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_images_labels\n\u001b[1;32m      4\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;66;03m# k-hot value\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# load train\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataload'"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "from dataload import read_images_labels\n",
    "\n",
    "k = 10 # k-hot value\n",
    "\n",
    "# load train\n",
    "training_images_filepath = './data/train-images-idx3-ubyte/train-images-idx3-ubyte'\n",
    "training_labels_filepath = './data/train-labels-idx1-ubyte/train-labels-idx1-ubyte'\n",
    "x_train, y_train = read_images_labels(training_images_filepath, training_labels_filepath)\n",
    "\n",
    "# load test\n",
    "test_images_filepath = './data/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte'\n",
    "test_labels_filepath = './data/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte'\n",
    "x_test, y_test = read_images_labels(test_images_filepath, test_labels_filepath)\n",
    "\n",
    "print(np.array(x_train[45854]).reshape(28,28))\n",
    "print(np.array(y_train[45854]))\n",
    "\n",
    "# reformat data for model\n",
    "x_train = x_train.transpose() * (1/255)\n",
    "\n",
    "# data dimension D\n",
    "D = x_train.shape[0]\n",
    "assert D==784\n",
    "# number of training examples N\n",
    "N = x_train.shape[1]\n",
    "\n",
    "# reformat data to k-hot format TODO: does numpy have a better way to do this?\n",
    "temp_array = np.zeros((k, N))\n",
    "for index,val in enumerate(y_train):\n",
    "    temp_array[val][index] = 1\n",
    "print(temp_array[:, 45854])\n",
    "y_train = temp_array\n",
    "\n",
    "x_test = x_test.transpose() * (1/255)\n",
    "y_test = y_test.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris dataset\n",
    "\n",
    "k = 3 # k-hot value\n",
    "\n",
    "# csv columns are sepal_length,sepal_width,petal_length,petal_width,species\n",
    "iris_dataset = np.genfromtxt('iris.csv', delimiter=',')\n",
    "\n",
    "# randomize and split\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(iris_dataset)\n",
    "\n",
    "data = iris_dataset[:, [range(4)]]\n",
    "labels = iris_dataset[:, 4]\n",
    "\n",
    "data = np.reshape(data, (150,4))\n",
    "\n",
    "x_train = data[[range(iris_dataset.shape[0]//5*3)]]\n",
    "x_train = x_train.transpose()\n",
    "x_test = data[[range(iris_dataset.shape[0]//5*3, iris_dataset.shape[0])]]\n",
    "x_test = x_test.transpose()\n",
    "\n",
    "# data dimension D\n",
    "D = x_train.shape[0]\n",
    "assert D==4\n",
    "# number of training examples N\n",
    "N = x_train.shape[1]\n",
    "\n",
    "labels_train = labels[[range(iris_dataset.shape[0]//5*3)]]\n",
    "\n",
    "# format\n",
    "# reformat data to k-hot format TODO: does numpy have a better way to do this?\n",
    "y_train = np.zeros((k, N))\n",
    "for index, val in enumerate(labels_train): y_train[int(val)][index] = 1\n",
    "\n",
    "y_test = labels[iris_dataset.shape[0]//5*3:iris_dataset.shape[0]].reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.437 -0.096  0.834 -1.575 -1.011 -1.3    0.325  0.512 -0.379  0.094 -0.93   0.224  1.136  1.052  0.033]\n",
      " [-0.506 -1.388  1.11  -1.15   0.161  0.736  0.645 -0.695 -0.336 -0.827 -0.217  1.464  0.434 -0.172 -1.058]\n",
      " [-0.486  0.283  0.504  1.224  0.253  0.422  0.366  0.896  1.678 -0.459  0.277 -2.194  0.991  0.097  0.211]\n",
      " [-0.124  0.199  0.853 -0.087 -1.5   -0.799  1.62   0.828  1.223  0.501 -0.074 -0.528 -0.508 -1.332 -0.225]\n",
      " [ 0.549 -1.225 -0.797  0.206  0.42  -0.728  0.386 -0.58   1.944  0.425  0.326  0.231 -1.7    0.45  -1.626]\n",
      " [ 0.999  0.276 -0.412  0.47   1.203  2.483  1.693 -0.393 -0.514  0.253  0.851  1.979  0.391 -1.124 -0.565]\n",
      " [ 0.395  0.669  0.355  1.844  1.508 -1.087  0.444 -1.453 -0.979 -0.715 -0.028 -1.421  0.203  0.429 -0.149]\n",
      " [-0.717  0.605  0.424 -0.467  1.664  1.768 -1.275 -0.025 -1.732  0.754  1.037  0.67  -0.171 -0.809 -1.07 ]\n",
      " [ 1.628  0.442  0.073 -0.721  0.263  1.139  0.203 -0.762 -0.177  0.789  1.833 -0.364 -1.891 -0.381  0.637]\n",
      " [ 0.241 -1.358  0.348  0.59  -0.482 -1.489 -0.501  1.452  0.114  0.153  0.473  2.314  0.92   0.38  -0.064]]\n",
      "Cost at epoch 0: 0.033708453045746926\n"
     ]
    }
   ],
   "source": [
    "### instatiate model architecture\n",
    "\n",
    "# architecture hyperparameters\n",
    "hidden_dim = 15 # width of hidden layer\n",
    "\n",
    "# instatiate first matrix of weights\n",
    "w_2 = np.random.normal(loc=0, scale=1, size=(hidden_dim,D))\n",
    "# instatiate first bias vector\n",
    "b_2 = np.random.normal(loc=0, scale=1, size=(hidden_dim,1))\n",
    "\n",
    "# instatiate second set of weights\n",
    "w_3 = np.random.normal(loc=0, scale=1, size=(k,hidden_dim))\n",
    "# instatiate second set of weights\n",
    "b_3 = np.random.normal(loc=0, scale=1, size=(k,1))\n",
    "\n",
    "print(w_3)\n",
    "\n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x): \n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1-sig)\n",
    "\n",
    "# implementation of softmax from: https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python \n",
    "def softmax(x): \n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "### training\n",
    "\n",
    "# training hyperparamters\n",
    "alpha = 0.1 # learning rate\n",
    "epochs = 3 # number of epochs\n",
    "batch_size = 5\n",
    "\n",
    "def extrem(array, array_name): return f'Extremal Values for {array_name}: {np.amin(array), np.amax(array)}'\n",
    "\n",
    "def loss(activation, label): return 0.5*(activation-label)**2\n",
    "\n",
    "def cost(loss_vec): return np.average(loss_vec)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_index in range(int(N/batch_size)): # TODO look at this again\n",
    "        x_train_batch = x_train[:, [batch_index, batch_index + batch_size]] \n",
    "        y_train_batch = y_train[:, [batch_index, batch_index + batch_size]]\n",
    "\n",
    "        # forward pass\n",
    "        z_2 = w_2.dot(x_train_batch) + b_2.dot(np.ones((1,x_train_batch.shape[1])))\n",
    "        a_2 = sigmoid(z_2) \n",
    "        z_3 = w_3.dot(a_2) + b_3.dot(np.ones((1,a_2.shape[1])))\n",
    "        a_3 = sigmoid(z_3)\n",
    "\n",
    "        # backprop\n",
    "        delta_3 = np.multiply((a_3-y_train_batch), sigmoid_prime(z_3))\n",
    "        delta_2 = np.multiply(w_3.transpose().dot(delta_3), sigmoid_prime(z_2))\n",
    "\n",
    "        w_3 -= alpha*(1/batch_size)*delta_3.dot(a_2.transpose())\n",
    "        b_3 -= alpha*np.reshape(np.mean(delta_3, axis=1), (-1,1))\n",
    "\n",
    "        w_2 -= alpha*(1/batch_size)*delta_2.dot(x_train_batch.transpose())\n",
    "        b_2 -= alpha*np.reshape(np.mean(delta_2, axis=1), (-1,1))\n",
    "        # if batch_index % 10 == 0: print(f\"Batch {batch_index} completed.\")\n",
    "\n",
    "    # final foward pass\n",
    "    a_2 = sigmoid(w_2.dot(x_train) + b_2.dot(np.ones((1,x_train.shape[1]))))\n",
    "    a_3 = sigmoid(w_3.dot(a_2) + b_3.dot(np.ones((1,x_train.shape[1]))))\n",
    "\n",
    "    loss_val = loss(a_3, y_train)\n",
    "    cost_val = cost(loss_val)\n",
    "    if epoch % 100 == 0: print(f\"Cost at epoch {epoch}: {cost_val}\")\n",
    "    # print(f\"Cost at epoch {epoch}: {cost_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919 of 10000 wrong \n",
      "Accuracy: 0.9081\n"
     ]
    }
   ],
   "source": [
    "### testing\n",
    "\n",
    "# test dimension D_test\n",
    "D_test = x_test.shape[1]\n",
    "\n",
    "test_ones_1 = np.ones((1,D_test))\n",
    "test_ones_2 = np.ones((1,D_test))\n",
    "\n",
    "# forward pass on test data\n",
    "test_a_2 = sigmoid(w_2.dot(x_test) + b_2.dot(np.ones((1,x_test.shape[1]))))\n",
    "test_a_3 = sigmoid(w_3.dot(test_a_2) + b_3.dot(np.ones((1,x_test.shape[1]))))\n",
    "predictions = np.argmax(test_a_3, axis=0)\n",
    "\n",
    "num_wrong = np.sum(np.not_equal(y_test, predictions))\n",
    "test_size = y_test.shape[1]\n",
    "print(f\"{num_wrong} of {test_size} wrong \\nAccuracy: {1 - (num_wrong/test_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "test_a_2 = sigmoid(w_2.dot(x_train[:, [0]]) + b_2.dot(np.ones((1,x_train[:, [0]].shape[1]))))\n",
    "test_a_3 = sigmoid(w_3.dot(test_a_2) + b_3.dot(np.ones((1,x_train[:, [0]].shape[1]))))\n",
    "predictions = np.argmax(test_a_3, axis=0)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost = 0.21236912276027578\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n",
      "Cost = 0.049999999998823805\n"
     ]
    }
   ],
   "source": [
    "# using custom network class\n",
    "from model import Network\n",
    "\n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x): \n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1-sig)\n",
    "\n",
    "# define loss & cost function and derivative\n",
    "def mse_loss(activation, label): return 0.5*(activation-label)**2\n",
    "def mse_loss_prime(activation, label): return (activation-label)\n",
    "\n",
    "def cost(loss, activation, label): return np.average(loss(activation, label))\n",
    "# def cost(loss): return np.average(loss)\n",
    "\n",
    "network = Network(dims=(784,15,10), activation_funcs = [(sigmoid, sigmoid_prime),(sigmoid, sigmoid_prime)], loss=(mse_loss, mse_loss_prime), cost=cost, seed=1)\n",
    "\n",
    "network.train(train_data=x_train, labels=y_train, batch_size=1, learning_rate=0.01, epochs=20, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compare to\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,hidden_dim, dtype=torch.float64),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(hidden_dim,10, dtype=torch.float64),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train.transpose())\n",
    "y_train_tensor = torch.from_numpy(y_train.transpose())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(x_train_tensor)\n",
    "    loss = loss_fn(y_pred, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "x_test_tensor = torch.from_numpy(x_test.transpose())\n",
    "\n",
    "y_test_t = model(x_test_tensor)\n",
    "predictions = torch.argmax(y_test_t, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
